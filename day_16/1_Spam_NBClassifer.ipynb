{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET :SMS Spam Collection :The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.  Content The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.  This corpus has been collected from free or free for research sources at the Internet:   A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read  file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\amgupta\\\\OneDrive - Microsoft\\\\Python_Temp\\\\16\\\\TA\\\\spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set description and visualize ham and spam count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1da217b94e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df, x='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To create spam filter : \n",
    "### Clean and Normalize text Convert text into vectors (using bag of words model) that machine learning models can understand.\n",
    "Train and test Classifier Clean and normalize text \n",
    "It will be done in following steps:  \n",
    "- Remove punctuations \n",
    "- Remove all stopwords \n",
    "- Apply stemming (converting to normal form of word).  \n",
    "  For example, 'driving car' and 'drives car' becomes drive car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a method to return normailzed text in form of tokens (lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer as Stemmer\n",
    "def process(text):\n",
    "    # lowercase it\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = ''.join([t for t in text if t not in string.punctuation])\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [t for t in text.split() if t not in stopwords.words('english')]\n",
    "    \n",
    "    # stemming\n",
    "    st = Stemmer()\n",
    "    text = [st.stem(t) for t in text]\n",
    "    \n",
    "    # return token list\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'hyderabad']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process('I am going to Hyderabad !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holiday', 'play', 'cricket', 'jeff', 'play', 'well']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "process('It\\'s holiday and we are playing cricket. Jeff is playing very well!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [go, jurong, point, crazi, avail, bugi, n, gre...\n",
       "1                          [ok, lar, joke, wif, u, oni]\n",
       "2     [free, entri, 2, wkli, comp, win, fa, cup, fin...\n",
       "3         [u, dun, say, earli, hor, u, c, alreadi, say]\n",
       "4     [nah, dont, think, goe, usf, live, around, tho...\n",
       "5     [freemsg, hey, darl, 3, week, word, back, id, ...\n",
       "6     [even, brother, like, speak, treat, like, aid,...\n",
       "7     [per, request, mell, mell, oru, minnaminungint...\n",
       "8     [winner, valu, network, custom, select, receiv...\n",
       "9     [mobil, 11, month, u, r, entitl, updat, latest...\n",
       "10    [im, gonna, home, soon, dont, want, talk, stuf...\n",
       "11    [six, chanc, win, cash, 100, 20000, pound, txt...\n",
       "12    [urgent, 1, week, free, membership, å£100000, ...\n",
       "13    [ive, search, right, word, thank, breather, pr...\n",
       "14                                       [date, sunday]\n",
       "15    [xxxmobilemovieclub, use, credit, click, wap, ...\n",
       "16                                     [oh, kim, watch]\n",
       "17    [eh, u, rememb, 2, spell, name, ye, v, naughti...\n",
       "18    [fine, thatåõ, way, u, feel, thatåõ, way, gota...\n",
       "19    [england, v, macedonia, dont, miss, goalsteam,...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with our dataset\n",
    "df['message'][:20].apply(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert each message to vectors that machine learning models can understand. \n",
    "\n",
    "We will do that using bag-of-words model  \n",
    "We will use TfidfVectorizer. It will convert collection of text documents (SMS corpus) into 2D matrix.  One dimension represent documents and other dimension repesents each unique word in SMS corpus . .  \n",
    "\n",
    "If nth term t has occured p times in mth document, (m, n) value in this matrix will be TF-IDF(t),   \n",
    "\n",
    "where [TF-IDF(t)](https://en.wikipedia.org/wiki/Tf–idf) = Term Frequency (TF) * Inverse Document Frequency (IDF)  Term Frequency (TF) is a measure of how frequent a term occurs in a document.  TF(t)= Number of times term t appears in document (p) / Total number of terms in that document  Inverse Document Frequency (IDF) is measure of how important term is. \n",
    "\n",
    "For TF, all terms are equally treated. But, in IDF, for words that occur frequently like 'is' 'the' 'of' are assigned less weight. While terms that occur rarely that can easily help identify class of input features will be weighted high.  Inverse Document Frequency, IDF(t)= loge(Total number of documents / Number of documents with term t in it)  At end we will have for every message, vectors normalized to unit length equal to size of vocalbulary (number of unique terms from entire SMS corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform SMS corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = TfidfVectorizer(analyzer=process)\n",
    "data = tfidfv.fit_transform(df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5163)\t0.14927789518864504\n",
      "  (0, 3099)\t0.2619594860603665\n",
      "  (0, 3556)\t0.1823936981947932\n",
      "  (0, 2301)\t0.2799722422538804\n",
      "  (0, 517)\t0.2028231783430636\n",
      "  (0, 7615)\t0.17888800689645606\n",
      "  (0, 7771)\t0.20844580948798538\n",
      "  (0, 1342)\t0.17197197887512358\n",
      "  (0, 3750)\t0.22739787806278594\n",
      "  (0, 4322)\t0.1540953727694651\n",
      "  (0, 3151)\t0.2285395329800025\n",
      "  (0, 6666)\t0.170731031846155\n",
      "  (0, 6901)\t0.29740977983510175\n",
      "  (0, 7901)\t0.22739787806278594\n",
      "  (0, 6643)\t0.2716934251981628\n",
      "  (0, 1929)\t0.3163618484099023\n",
      "  (0, 6195)\t0.1605491895946896\n",
      "  (0, 8044)\t0.23349366212342992\n",
      "  (0, 5826)\t0.3163618484099023\n",
      "  (1, 4322)\t0.4308131677112723\n",
      "  (1, 2795)\t0.26700632601835994\n",
      "  (1, 1669)\t0.34131563987319075\n",
      "  (1, 6541)\t0.3133619420394793\n",
      "  (1, 7215)\t0.34384601902147\n",
      "  (1, 1032)\t0.4422353754267648\n",
      "  :\t:\n",
      "  (13, 7289)\t0.12521546275515694\n",
      "  (13, 2974)\t0.24670261463550688\n",
      "  (13, 6992)\t0.7012811905405189\n",
      "  (13, 7590)\t0.42903189940026254\n",
      "  (13, 2935)\t0.2205724439373421\n",
      "  (13, 3326)\t0.38013858109567755\n",
      "  (13, 1325)\t0.23331884846313858\n",
      "  (14, 7276)\t0.12657399571517822\n",
      "  (14, 2546)\t0.11520387198184155\n",
      "  (14, 373)\t0.1596683100386687\n",
      "  (14, 7422)\t0.16256182050322743\n",
      "  (14, 2734)\t0.41071151847258835\n",
      "  (14, 4494)\t0.2504878996833017\n",
      "  (14, 4710)\t0.13836965379822044\n",
      "  (14, 3277)\t0.2504878996833017\n",
      "  (14, 4957)\t0.18887294240422406\n",
      "  (14, 7382)\t0.1093319848113401\n",
      "  (14, 4901)\t0.19799849447898876\n",
      "  (14, 6914)\t0.2116131898708974\n",
      "  (14, 832)\t0.43936084208752585\n",
      "  (14, 2679)\t0.19799849447898876\n",
      "  (14, 7241)\t0.2504878996833017\n",
      "  (14, 6145)\t0.2310505447770996\n",
      "  (14, 641)\t0.2504878996833017\n",
      "  (14, 5524)\t0.2504878996833017\n"
     ]
    }
   ],
   "source": [
    "print(data[5:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check what values it gives for a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n"
     ]
    }
   ],
   "source": [
    "mess = df.iloc[2]['message']\n",
    "print(mess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7741)\t0.18906287739887084\n",
      "  (0, 7708)\t0.14471405235314777\n",
      "  (0, 7276)\t0.12336193745345178\n",
      "  (0, 7099)\t0.2190885570936267\n",
      "  (0, 6959)\t0.11759458460817876\n",
      "  (0, 5856)\t0.16027970945850903\n",
      "  (0, 5815)\t0.2330497030932461\n",
      "  (0, 5768)\t0.2330497030932461\n",
      "  (0, 4592)\t0.15903719770411495\n",
      "  (0, 3091)\t0.11505037200973967\n",
      "  (0, 2969)\t0.16669800498830506\n",
      "  (0, 2868)\t0.4660994061864922\n",
      "  (0, 2748)\t0.3571909758763146\n",
      "  (0, 2246)\t0.20302402339849024\n",
      "  (0, 2076)\t0.19516151371199045\n",
      "  (0, 1180)\t0.16669800498830506\n",
      "  (0, 833)\t0.2190885570936267\n",
      "  (0, 433)\t0.22518719340674634\n",
      "  (0, 420)\t0.22518719340674634\n",
      "  (0, 413)\t0.09987750376879972\n",
      "  (0, 72)\t0.2330497030932461\n"
     ]
    }
   ],
   "source": [
    "print(tfidfv.transform([mess]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For better View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\tidf\ttfidf\tterm\n",
      "72 8.5271 0.2330 08452810075over18\n",
      "413 3.6544 0.0999 2\n",
      "420 8.2394 0.2252 2005\n",
      "433 8.2394 0.2252 21st\n",
      "833 8.0163 0.2191 87121\n",
      "1180 6.0993 0.1667 appli\n",
      "2076 7.1408 0.1952 comp\n",
      "2246 7.4285 0.2030 cup\n",
      "2748 6.5346 0.3572 entri\n",
      "2868 8.5271 0.4661 fa\n",
      "2969 6.0993 0.1667 final\n",
      "3091 4.2096 0.1151 free\n",
      "4592 5.8190 0.1590 may\n",
      "5768 8.5271 0.2330 questionstd\n",
      "5815 8.5271 0.2330 ratetc\n",
      "5856 5.8645 0.1603 receiv\n",
      "6959 4.3027 0.1176 text\n",
      "7099 8.0163 0.2191 tkt\n",
      "7276 4.5137 0.1234 txt\n",
      "7708 5.2950 0.1447 win\n",
      "7741 6.9176 0.1891 wkli\n"
     ]
    }
   ],
   "source": [
    "j = tfidfv.transform([mess]).toarray()[0]\n",
    "print('index\\tidf\\ttfidf\\tterm')\n",
    "for i in range(len(j)):\n",
    "    if j[i] != 0:\n",
    "        print(i, format(tfidfv.idf_[i], '.4f'), format(j[i], '.4f'), tfidfv.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having messages in form of vectors, we are ready to train our classifier.  We will use Naive Bayes which is well known classifier while working with text data.  Before that we will use pipeline feature of sklearn to create a pipeline of TfidfVectorizer followed by Classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input will be message passed to first stage TfidfVectorizer which will transform it and pass it to Naive Bayes Classifier to get output label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_filter = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(analyzer=process)), # messages to weighted TFIDF score\n",
    "    ('classifier', MultinomialNB())                    # train on TFIDF vectors with Naive Bayes\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.20, random_state = 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train spam_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer=<function process at 0x000001E2D47AA950>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), norm='l2'...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_filter.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = spam_filter.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test cases 1115\n",
      "Number of wrong of predictions 39\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test.iloc[i] != predictions[i]:\n",
    "        count += 1\n",
    "print('Total number of test cases', len(y_test))\n",
    "print('Number of wrong of predictions', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for wrong predictions that were classified as ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419     Send a logo 2 ur lover - 2 names joined by a h...\n",
       "3139    sexy sexy cum and text me im wet and warm and ...\n",
       "3790    Twinks, bears, scallies, skins and jocks are c...\n",
       "2877    Hey Boys. Want hot XXX pics sent direct 2 ur p...\n",
       "2377    YES! The only place in town to meet exciting a...\n",
       "1499    SMS. ac JSco: Energy is high, but u may not kn...\n",
       "3417    LIFE has never been this much fun and great un...\n",
       "3358    Sorry I missed your call let's talk when you h...\n",
       "2412    I don't know u and u don't know me. Send CHAT ...\n",
       "3862    Oh my god! I've found your number again! I'm s...\n",
       "659     88800 and 89034 are premium phone services cal...\n",
       "3109    Good Luck! Draw takes place 28th Feb 06. Good ...\n",
       "5466    http//tms. widelive.com/index. wml?id=820554ad...\n",
       "1268    Can U get 2 phone NOW? I wanna chat 2 set up m...\n",
       "491     Congrats! 1 year special cinema pass for 2 is ...\n",
       "2246    Hi ya babe x u 4goten bout me?' scammers getti...\n",
       "2828    Send a logo 2 ur lover - 2 names joined by a h...\n",
       "3528    Xmas & New Years Eve tickets are now on sale f...\n",
       "4247    accordingly. I repeat, just text the word ok o...\n",
       "4142    In The Simpsons Movie released in July 2007 na...\n",
       "3979                                   ringtoneking 84484\n",
       "1637    0A$NETWORKS allow companies to bill for SMS, s...\n",
       "2802                    FreeMsg>FAV XMAS TONES!Reply REAL\n",
       "3270    You have 1 new voicemail. Please call 08719181...\n",
       "2294     You have 1 new message. Please call 08718738034.\n",
       "2619    <Forwarded from 21870000>Hi - this is your Mai...\n",
       "234     Text & meet someone sexy today. U can find a d...\n",
       "760     Romantic Paris. 2 nights, 2 flights from å£79 ...\n",
       "138     You'll not rcv any more msgs from the chat svc...\n",
       "689     <Forwarded from 448712404000>Please CALL 08712...\n",
       "879     U have a Secret Admirer who is looking 2 make ...\n",
       "1216    You have 1 new voicemail. Please call 08719181...\n",
       "1892    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
       "2351    Download as many ringtones as u like no restri...\n",
       "1317    Win the newest ÛÏHarry Potter and the Order o...\n",
       "4458    Welcome to UK-mobile-date this msg is FREE giv...\n",
       "1879    U have a secret admirer who is looking 2 make ...\n",
       "4309    Someone U know has asked our dating service 2 ...\n",
       "1673    Monthly password for wap. mobsi.com is 391784....\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[y_test != predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use classification report to get more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      0.96      0.98      1014\n",
      "       spam       0.72      1.00      0.84       101\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at precision column (for ham, it is 1.00), we can say that all number of wrong predictions (in output of [18]) came from spam predicted as ham. It is ok and cost of predicting spam as ham is negligible to that of predicting ham as spam.  Function to predict whether passed message is ham or spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_spam(s):\n",
    "    return spam_filter.predict([s])[0]\n",
    "\n",
    "detect_spam('Your cash-balance is currently 500 pounds - to maximize your cash-in now, send COLLECT to 83600.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

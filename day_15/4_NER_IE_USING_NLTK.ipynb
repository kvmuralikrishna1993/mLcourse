{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction using NLTK\n",
    "\n",
    "[NLTK](http://www.nltk.org/) is a python platform for natural language processing and information extraction. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, and an active discussion forum.\n",
    "\n",
    "The associated book, [Natural Language Processing with Python](http://www.nltk.org/book/) is also freely available and is a great resource both for NLP concepts and for practical examples of how to use the NLTK package.\n",
    "\n",
    "To get started, you need to install NLTK:\n",
    "    \n",
    "    sudo pip install -U nltk\n",
    "    \n",
    "After running the first command below (download()), you will be presented with a window to select which 'data' to download. For space reasons, only download 'book' (Everything used in the NLTK book). This in itself is a few hundred MB download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\amgupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\amgupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################################################\n",
    "# Purpose: NER (Named Entity Recognition) Information Extraction using NLTK       #\n",
    "# input:   Sample sentence and NLTK corpus                                        #\n",
    "# output:  POS and NER Tree                                                       #\n",
    " #\n",
    "###################################################################################\n",
    "\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN'),\n",
       " (',', ','),\n",
       " ('Arthur', 'NNP'),\n",
       " ('did', 'VBD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('feel', 'VB'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it is working\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning, Arthur didn't feel very good.\"\"\"\n",
    "#sentence = \"one hundred and twenty five\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " ',',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two commands tokenize the string, and tag each of the token with the part-of-speech. Here is a listing of tagsets that NLTK uses here (there are different tagsets used by different corpora).\n",
    "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following set of commands extracts named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'NN\\'), (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'), (\\',\\', \\',\\'), Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]), (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'), (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')])'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\amgupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Visual representation of the tree \n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "entities.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Named Entity Recognition \n",
    "\n",
    "As a somewhat more elaborate example, the following sequence of commands reads data from a file, and does NER on each of the sentences in the file. It doesn't do a very good job on this article, but in general, it seems to work quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  </JJ\n",
      "  html/NN\n",
      "  >/NNP\n",
      "  </NNP\n",
      "  body/NN\n",
      "  >/NNP\n",
      "  </NNP\n",
      "  p/NN\n",
      "  >/VBD\n",
      "  The/DT\n",
      "  1989/CD\n",
      "  Tour/NNP\n",
      "  de/FW\n",
      "  (GPE France/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  76th/CD\n",
      "  edition/NN\n",
      "  of/IN\n",
      "  one/CD\n",
      "  of/IN\n",
      "  cycling/NN\n",
      "  's/POS\n",
      "  (FACILITY Grand/NNP Tours/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "with open(\"news1.html\", \"r\") as myfile:\n",
    "    data = myfile.read()   \n",
    "    \n",
    "sentences = nltk.sent_tokenize(data)\n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "entities0 = nltk.ne_chunk(sentences[0])\n",
    "print(entities0)\n",
    "entities0.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\amgupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Tree.draw of Tree('S', [Tree('NP-SBJ', [Tree('NP', [Tree('NNP', ['Pierre']), Tree('NNP', ['Vinken'])]), Tree(',', [',']), Tree('ADJP', [Tree('NP', [Tree('CD', ['61']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])]), Tree(',', [','])]), Tree('VP', [Tree('MD', ['will']), Tree('VP', [Tree('VB', ['join']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['board'])]), Tree('PP-CLR', [Tree('IN', ['as']), Tree('NP', [Tree('DT', ['a']), Tree('JJ', ['nonexecutive']), Tree('NN', ['director'])])]), Tree('NP-TMP', [Tree('NNP', ['Nov.']), Tree('CD', ['29'])])])]), Tree('.', ['.'])])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for better Visual of tree\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Extraction\n",
    "\n",
    "The second key task is to extract relations between entities. The following code snippet finds the relations between organizations and locations, in one of the existing datasets in NLTK. See the book webpage for more details on the regular expression pattern below: http://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

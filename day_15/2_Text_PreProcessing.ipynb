{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing for Text Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "# Purpose: Basic understanding on Text Pre-Processing                             #\n",
    "# input:   strings of text or a sentence                                          #\n",
    "# output:  Cleaned sentence                                                       #\n",
    " #     Wordnet is dictionary                                 #\n",
    "###################################################################################\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing: Convert to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'convert this into lower case.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_lower(input_sentence):\n",
    "    words = input_sentence.split()\n",
    "    clean_words = [x.lower() for x in words]\n",
    "    clean_sentence = \" \".join(clean_words) \n",
    "    return clean_sentence\n",
    "\n",
    "sentence = \"Convert This Into Lower Case.\"\n",
    "\n",
    "to_lower(sentence)\n",
    "# We don't have to implement to_lower method. lower() works on sentences too. The above is for demo of function creations.\n",
    "# sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing: Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This Sample test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"This is A Sample test ...\"\n",
    "stop_words = [\"is\", \"a\", \"A\",\"this\", \"...\"] \n",
    "\n",
    "def remove_stop_words(input_sentence):\n",
    "    words = input_sentence.split()\n",
    "    clean_words = [i for i in words if i not in stop_words]\n",
    "    clean_sentence = \" \".join(clean_words) \n",
    "    return clean_sentence\n",
    "\n",
    "remove_stop_words(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Pre-Processing: Remove other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove in a sentence \n"
     ]
    }
   ],
   "source": [
    "#Remove other words (such as hashtags)\n",
    "\n",
    "import re\n",
    "sentence = \"remove #tagwords in a #IIITHYD sentence #AVC\"\n",
    "pattern = \"#[\\w]*\"\n",
    "\n",
    "clean_sentence = sentence\n",
    "iter = re.finditer(pattern, sentence)\n",
    "for i in iter:\n",
    "    #print(i, i.group().strip())  #i.group gives the matching string in current iteration\n",
    "    clean_sentence = re.sub(i.group(), '', clean_sentence)\n",
    "clean_sentence = re.sub(' +', ' ', clean_sentence)\n",
    "#clean_sentence = clean_sentence.replace(\"  \",\" \")\n",
    "print(clean_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing: Standardization: Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get lookup error in code then you need to download wordnet.. etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studied\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#word = \"consigning\" \n",
    "#word = \"played\" \n",
    "word = \"studied\" \n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "print(lem.lemmatize(word))\n",
    "\n",
    "stem = PorterStemmer()\n",
    "print(stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
